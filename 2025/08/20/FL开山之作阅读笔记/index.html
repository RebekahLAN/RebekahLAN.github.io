<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>FL开山之作阅读笔记 | RebekahLAN</title><meta name="author" content="RebekahLAN"><meta name="copyright" content="RebekahLAN"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="ffffff"><meta name="description" content="出于科研训练的需要，煮波需要深入阅读这篇联邦学习的开山之作，补补基础知识，并学习一下如何阅读文献。 跳楼了，实验细节根本看不懂😋🔫。 参考： (11 封私信 &#x2F; 80 条消息) 联邦学习开山之作：Communication-Efﬁcient Learning of Deep Networks from Decentralized Data学习笔记 - 知乎 (11 封私信 &amp;#x2F">
<meta property="og:type" content="article">
<meta property="og:title" content="FL开山之作阅读笔记">
<meta property="og:url" content="https://rebekahlan.github.io/2025/08/20/FL%E5%BC%80%E5%B1%B1%E4%B9%8B%E4%BD%9C%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/index.html">
<meta property="og:site_name" content="RebekahLAN">
<meta property="og:description" content="出于科研训练的需要，煮波需要深入阅读这篇联邦学习的开山之作，补补基础知识，并学习一下如何阅读文献。 跳楼了，实验细节根本看不懂😋🔫。 参考： (11 封私信 &#x2F; 80 条消息) 联邦学习开山之作：Communication-Efﬁcient Learning of Deep Networks from Decentralized Data学习笔记 - 知乎 (11 封私信 &amp;#x2F">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://rebekahlan.github.io/images/background.jpg">
<meta property="article:published_time" content="2025-08-20T06:33:55.000Z">
<meta property="article:modified_time" content="2025-08-21T03:36:41.458Z">
<meta property="article:author" content="RebekahLAN">
<meta property="article:tag" content="Federated Learning">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://rebekahlan.github.io/images/background.jpg"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "FL开山之作阅读笔记",
  "url": "https://rebekahlan.github.io/2025/08/20/FL%E5%BC%80%E5%B1%B1%E4%B9%8B%E4%BD%9C%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/",
  "image": "https://rebekahlan.github.io/images/background.jpg",
  "datePublished": "2025-08-20T06:33:55.000Z",
  "dateModified": "2025-08-21T03:36:41.458Z",
  "author": [
    {
      "@type": "Person",
      "name": "RebekahLAN",
      "url": "https://rebekahlan.github.io"
    }
  ]
}</script><link rel="shortcut icon" href="/images/star.png"><link rel="canonical" href="https://rebekahlan.github.io/2025/08/20/FL%E5%BC%80%E5%B1%B1%E4%B9%8B%E4%BD%9C%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', 'ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: 'Copy Successful',
    error: 'Copy Failed',
    noSupport: 'Browser Not Supported'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: 'Just now',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: 'Load More'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'FL开山之作阅读笔记',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><link rel="stylesheet" href="/css/custom.css"><meta name="generator" content="Hexo 7.3.0"></head><body><div id="web_bg" style="background: transparent;"></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url(/images/background.jpg);"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">RebekahLAN</span></a><a class="nav-page-title" href="/"><span class="site-name">FL开山之作阅读笔记</span><span class="site-name"><i class="fa-solid fa-circle-arrow-left"></i><span>  Back to Home</span></span></a></span><div id="menus"></div></nav><div id="post-info"><h1 class="post-title">FL开山之作阅读笔记</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2025-08-20T06:33:55.000Z" title="Created 2025-08-20 14:33:55">2025-08-20</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2025-08-21T03:36:41.458Z" title="Updated 2025-08-21 11:36:41">2025-08-21</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Learning/">Learning</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post Views:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><p>出于科研训练的需要，煮波需要深入阅读这篇联邦学习的开山之作，补补基础知识，并学习一下如何阅读文献。</p>
<p>跳楼了，实验细节根本看不懂😋🔫。</p>
<p>参考：</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/445458807">(11 封私信 &#x2F; 80 条消息) 联邦学习开山之作：Communication-Efﬁcient Learning of Deep Networks from Decentralized Data学习笔记 - 知乎</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/537953950">(11 封私信 &#x2F; 80 条消息) 联邦学习原始论文解读 - 知乎</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/chrnhao/article/details/142751006">联邦学习论文精读—Communication-Efficient Learning of Deep Networks from Decentralized Data-CSDN博客</a></p>
<h1 id="标题"><a href="#标题" class="headerlink" title="标题"></a>标题</h1><p>Communication-Efficient Learning of Deep Networks from Decentralized Data</p>
<p>→从<strong>分散（去中心化）数据</strong>中进行深度网络的通信高效学习</p>
<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>传统的机器学习方法需要将<strong>所有</strong>用户数据上传到数据中心进行<strong>集中</strong>训练。但这有一些缺陷：</p>
<ol>
<li><strong>隐私风险</strong>如数据泄露、滥用；</li>
<li><strong>实际训练困难</strong>如数据量巨大，上传成本高）。</li>
</ol>
<p>因此提出联邦学习（Federated Learning）的概念。</p>
<p>提出的基于迭代模型平均的深度网络联合学习的实用方法，用5种模型架构和4个数据集验证得出该方法对于不平衡和非IID数据分布（这是该设置的定义特征）具有鲁棒性。</p>
<p>与传统的同步随机梯度下降相比，该方法将所需的通信轮次<strong>减少了10到100倍</strong>，极大地提升了通信效率。</p>
<p>补充：<strong>什么是IID（独立同分布）？</strong><br>输入空间X的所有样本服从一个隐含未知的分布，训练数据所有样本都是独立地从这个分布上采样而得。即是指一组随机变量中每个变量的概率分布都相同，且这些随机变量互相独立。在传统有监督机器学习研究里，IID是一个重要假设，因为人们希望训练集和测试集满足IID。<br>对于none-IID，即全部随机，数据集选取（label比例）也随机，测试机训练集划分也随机。</p>
<h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>手机、电脑等移动设备已成为主要的计算设备，其传感器能收集海量的、与用户行为高度相关的隐私数据，利用这些数据训练模型可以极大提升应用的智能化水平（如语音识别、智能输入法），但传统的“数据上传→集中训练”模式存在严重的<strong>隐私风险</strong>和<strong>通信成本</strong>问题。</p>
<h2 id="联邦学习的定义"><a href="#联邦学习的定义" class="headerlink" title="联邦学习的定义"></a>联邦学习的定义</h2><ul>
<li>用户数据永远存储在本地设备（客户端）上，不上传。</li>
<li><strong>中央服务器</strong>协调一个由设备组成的“联邦”，每个客户端使用<strong>本地数据</strong>计算对当前全局模型的<strong>更新</strong>，并将更新发送给中央服务器。</li>
<li>中央服务器聚合所有更新，形成新的全局模型，再下发给客户端，以此类推。</li>
<li>这一过程遵循了“最小化数据收集 (data minimization)”的隐私保护原则。</li>
</ul>
<p>需要信任中央服务器；联邦学习可以显著减少隐私安全风险，因为一些攻击是仅仅面向设备而不是面向云端。</p>
<h2 id="主要贡献"><a href="#主要贡献" class="headerlink" title="主要贡献"></a>主要贡献</h2><ol>
<li>将移动设备上分散数据的训练问题确立为一个重要的研究方向。</li>
<li>选择并提出了一种简单、实用的算法（即FedAvg联邦平均法）来解决此问题。</li>
<li>进行了广泛的实验，证明该方法对非IID和不平衡数据具有鲁棒性，并能将通信轮次减少一个数量级。</li>
</ol>
<h2 id="联邦学习适用属性"><a href="#联邦学习适用属性" class="headerlink" title="联邦学习适用属性"></a>联邦学习适用属性</h2><ol>
<li>与中心服务器提供的代理数据相比，使用来自移动设备的真实数据进行训练就有更明显的优势。</li>
<li>这些数据非常隐私，或者和模型的大小相比尺寸过大，所以为了贯彻集中收集原则，最好不要出于存粹于训练模型的目的将其记录到中心服务器。</li>
<li>对于有监督学习任务，数据的标签可以从用户的交互中自然推断出来。</li>
</ol>
<p>适用场景举例：</p>
<ol>
<li>根据用户手机相册预测哪些照片会被反复查看或分享。</li>
<li>根据用户在手机键盘上的输入，提升语音识别和智能输入（如预测下一个词）的准确性。</li>
</ol>
<h2 id="隐私"><a href="#隐私" class="headerlink" title="隐私"></a>隐私</h2><ul>
<li>为联邦学习传输的信息是改进特定模型所需的最小更新；</li>
<li>这些更新内容包含的有效信息不可能大于原始的训练数据；</li>
<li>联邦算法不需要知道更新的来源。</li>
</ul>
<h2 id="联邦优化"><a href="#联邦优化" class="headerlink" title="联邦优化"></a>联邦优化</h2><p>联邦优化有几个关键属性，使其与典型的分布式优化问题不同：</p>
<ol>
<li><strong>非独立同分布</strong>：任何特定的用户的本地数据的分布都不会代表群体分布；</li>
<li><strong>不平衡</strong>：每个客户端拥有的数据量不均衡；</li>
<li><strong>大规模分布</strong>：参与训练的客户端数量较大，而平均每个客户端的数据量相对较小；</li>
<li><strong>限制通信</strong>：移动设备经常离线&#x2F;连接速度慢&#x2F;连接费用高。</li>
</ol>
<p>同步更新方案：</p>
<ol>
<li>每轮通信开始时，服务器会随机选择 <code>C</code> 比例的客户端参与。</li>
<li>服务器将当前的全局模型状态（如参数 <code>w</code>）发送给这些客户端。</li>
<li>每个选中的客户端基于全局模型和自己的本地数据集进行计算，并将更新发回服务器。</li>
<li>服务器聚合这些更新，更新全局状态，进入下一轮。</li>
</ol>
<p>没看懂那一大堆公式在干嘛（悲</p>
<p>在数据中心优化中，<strong>计算成本是主要瓶颈</strong>，因此研究重点是利用GPU加速计算。而在联邦优化中，<strong>通信成本是主要瓶颈</strong>（上传带宽可能只有1MB&#x2F;s或更低）。</p>
<p>因此，本文的核心目标是：<strong>通过增加本地计算量，来减少所需的通信轮次</strong>。</p>
<p>实现这一目标有两种途径：</p>
<ol>
<li><strong>增加并行度</strong>：每轮让更多的客户端（<code>C</code>）同时工作。</li>
<li><strong>增加每个客户端的计算量</strong>：让每个客户端在两次通信之间进行更复杂的计算（例如，进行多轮本地训练，而不是只计算一次梯度）。</li>
</ol>
<p>最终的结论：<strong>增加每个客户端的计算量是获得速度提升的主要途径</strong>。</p>
<h2 id="之前的相关工作"><a href="#之前的相关工作" class="headerlink" title="之前的相关工作"></a>之前的相关工作</h2><p>较之之前工作改进的方面：</p>
<ul>
<li>考虑到数据不平衡和非独立同分布的样本；</li>
<li>客户端数量巨大、每个客户端数据量小；</li>
<li>考虑到通信受限。</li>
<li>FedAvg算法通过<strong>多轮迭代</strong>的模型平均，克服了一次性平均“在最坏情况下效果不比在单个客户端上训练的模型更好”的局限性，实现了更好的性能。</li>
</ul>
<h1 id="联邦学习平均算法"><a href="#联邦学习平均算法" class="headerlink" title="联邦学习平均算法"></a>联邦学习平均算法</h1><h2 id="FedSGD"><a href="#FedSGD" class="headerlink" title="FedSGD"></a>FedSGD</h2><p>联邦学习中的一个简单基线算法，其核心是<strong>每轮通信只进行一次梯度计算</strong>。</p>
<ol>
<li><strong>服务器初始化</strong>：服务器初始化全局模型参数 <code>w_t</code>。</li>
<li><strong>选择客户端</strong>：服务器随机选择 <code>C</code> 比例的客户端参与本轮训练。</li>
<li><strong>分发模型</strong>：服务器将当前的全局模型 <code>w_t</code> 发送给所有选中的客户端。</li>
<li><strong>客户端计算梯度</strong>：<ul>
<li>每个选中的客户端 <code>k</code> 收到 <code>w_t</code>。</li>
<li>客户端 <code>k</code> 使用<strong>其全部本地数据</strong>，计算出当前模型 <code>w_t</code> 在其本地数据集上的<strong>平均梯度</strong> <code>g_k</code>。</li>
<li>计算公式：<code>g_k = ∇F_k(w_t)</code>，其中 <code>F_k</code> 是客户端 <code>k</code> 的局部损失函数。</li>
</ul>
</li>
<li><strong>上传梯度</strong>：客户端 <code>k</code> 将计算出的梯度向量 <code>g_k</code> 上传给服务器。</li>
<li><strong>服务器聚合梯度</strong>：服务器收集所有返回的梯度 <code>g_k</code>，并根据各客户端的数据量 <code>n_k</code>进行加权平均，得到全局梯度 <code>g</code>。<ul>
<li>计算公式：<code>g = Σ (n_k / n) * g_k</code>。</li>
</ul>
</li>
<li><strong>服务器更新模型</strong>：服务器使用这个全局梯度 <code>g</code>和一个学习率 <code>η</code>，通过梯度下降法更新全局模型。<ul>
<li>更新公式：<code>w_&#123;t+1&#125; = w_t - η *g </code>。</li>
</ul>
</li>
<li><strong>循环</strong>：将 <code>w_&#123;t+1&#125;</code> 作为下一轮的初始模型，重复步骤7，直到模型收敛。</li>
</ol>
<h2 id="FedAvg"><a href="#FedAvg" class="headerlink" title="FedAvg"></a>FedAvg</h2><p>通过<strong>增加</strong>客户端的本地<strong>计算量</strong>来<strong>减少通信轮次</strong>，通信效率大幅提升。</p>
<ol>
<li><strong>服务器初始化</strong>：服务器初始化全局模型参数 <code>w_t</code>。</li>
<li><strong>选择客户端</strong>：服务器随机选择 <code>C</code> 比例的客户端参与本轮训练。</li>
<li><strong>分发模型</strong>：服务器将当前的全局模型 <code>w_t</code> 发送给所有选中的客户端。</li>
<li><strong>客户端进行多轮本地训练</strong>(ClientUpdate)：<ul>
<li>每个选中的客户端 <code>k</code> 收到 <code>w_t</code>，并将其作为本地训练的<strong>初始模型</strong>。</li>
<li>客户端 <code>k</code> 在其本地数据上，使用小批量随机梯度下降（SGD）进行 <code>E</code> 轮训练（<code>E</code> 个epoch）。</li>
<li>在每轮本地训练中，客户端会将本地数据划分为大小为 <code>B</code> 的小批量（minibatch），并基于这些小批量对模型进行多次迭代更新。</li>
<li>经过 <code>E</code> 轮本地训练后，客户端得到了一个更新后的本地模型 <code>w_&#123;k,t+1&#125;</code>。</li>
</ul>
</li>
<li><strong>上传模型</strong>：客户端 <code>k</code> 将其更新后的完整模型参数 <code>w_&#123;k,t+1&#125;</code> 上传给服务器。</li>
<li><strong>服务器聚合模型</strong>：服务器收集所有返回的本地模型 <code>w_&#123;k,t+1&#125;</code>，并根据各客户端的数据量 <code>n_k</code>进行加权平均，得到新的全局模型 <code>w_&#123;t+1&#125;</code>。<ul>
<li>（更新公式：<code>w_&#123;t+1&#125; = Σ (n_k / n) * w_&#123;k,t+1&#125;</code>）。</li>
</ul>
</li>
<li><strong>循环</strong>：将 <code>w_&#123;t+1&#125;</code> 作为下一轮的初始模型，重复步骤6，直到模型收敛。</li>
</ol>
<h2 id="FedAvg为什么有效？"><a href="#FedAvg为什么有效？" class="headerlink" title="FedAvg为什么有效？"></a>FedAvg为什么有效？</h2><p>图1的证明：</p>
<ul>
<li>在MNIST数据集上，训练两个模型 <code>w</code> 和 <code>w&#39;</code>。它们的初始参数相同，但分别在两个不同的、不重叠的600张图片子集上独立训练。</li>
<li>如果将这两个模型的参数进行线性插值（<code>θw + (1-θ)w&#39;</code>），并计算在<strong>完整MNIST训练集</strong>上的损失，会发现当 <code>θ=0.5</code>（即取平均）时，损失函数达到一个非常低的值，<strong>远低于任何一个单独模型 <code>w</code> 或 <code>w&#39;</code> 的损失</strong>。</li>
<li>这表明，即使两个模型是在不同的数据子集上训练的，将它们的参数进行平均，也能得到一个在整体数据上表现更好的模型。这为 FedAvg 的“<strong>模型参数平均</strong>”提供了直观的理论支持。FedAvg 每一轮的聚合，本质上就是对多个“专家”（每个客户端）的模型进行平均。</li>
</ul>
<h2 id="FedAvg-与-FedSGD-的关系"><a href="#FedAvg-与-FedSGD-的关系" class="headerlink" title="FedAvg 与 FedSGD 的关系"></a><strong>FedAvg 与 FedSGD 的关系</strong></h2><p>FedAvg 是一个参数化的算法家族，FedSGD 只是它的一个特例：</p>
<ul>
<li>当 <code>E=1</code> 且 <code>B=∞</code>（即使用全部本地数据作为一个batch）时，FedAvg 就退化为 FedSGD。</li>
<li>通过增加 <code>E</code> 或减小 <code>B</code>，FedAvg 增加了每个客户端在每轮通信中的计算量，从而<strong>显著减少了通信轮次</strong>。</li>
</ul>
<h1 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h1><h2 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h2><h3 id="MNIST手写数字识别"><a href="#MNIST手写数字识别" class="headerlink" title="MNIST手写数字识别"></a>MNIST手写数字识别</h3><ul>
<li>使用的两种模型：<ol>
<li><strong>MNIST 2NN</strong>：一个简单的多层感知机（MLP）。</li>
<li><strong>MNIST CNN</strong>：一个更复杂的卷积神经网络（CNN）。</li>
</ol>
</li>
<li>数据划分方式：<ul>
<li><strong>IID (独立同分布)</strong>：将数据<strong>完全打乱</strong>后，平均分配给100个客户端。每个客户端获得600个随机样本。这代表了<strong>理想化的、平衡的</strong>数据分布。</li>
<li><strong>非IID (非独立同分布)</strong>：将数据<strong>按数字标签排序</strong>后，分成200个碎片，然后给每个客户端分配2个碎片。这导致大多数客户端<strong>只拥有两种数字的样本</strong>（例如，一个客户端只有“1”和“2”的图片）。这是一种<strong>极端的、病理性的非IID划分</strong>，用于测试算法在最坏情况下的鲁棒性。</li>
</ul>
</li>
</ul>
<h3 id="莎士比亚文集"><a href="#莎士比亚文集" class="headerlink" title="莎士比亚文集"></a>莎士比亚文集</h3><ul>
<li><strong>模型</strong>：使用一个<strong>堆叠的字符级LSTM</strong>模型。</li>
<li>数据划分方式：<ul>
<li>客户端定义：每个“说话角色”（如哈姆雷特、奥赛罗）被视为一个独立的客户端。</li>
<li>数据特性：这个数据集具有天然的非IID和不平衡特性。<ul>
<li><strong>非IID</strong>：不同角色（用户）的台词风格、用词、习惯完全不同。</li>
<li><strong>不平衡</strong>：主角台词多，小角色台词少。</li>
</ul>
</li>
<li><strong>训练&#x2F;测试划分</strong>：按时间顺序划分，前80%的台词用于训练，后20%用于测试，这模拟了真实的时间序列数据。</li>
</ul>
</li>
</ul>
<h2 id="增加并行性"><a href="#增加并行性" class="headerlink" title="增加并行性"></a>增加并行性</h2><p>首先探究了影响FedAvg算法性能的第一个关键参数：<strong>客户端比例 <code>C</code></strong>。</p>
<ol>
<li><strong>目标</strong>：探究增加并行性（即每轮让更多的客户端同时工作）是否能有效减少通信轮次。</li>
<li><strong>方法</strong>：<ul>
<li>作者使用了<strong>Table 1</strong>（表1）中的数据来展示结果。该表记录了在不同 <code>C</code> 值下，MNIST 2NN和CNN模型达到特定目标准确率（97%和99%）所需的<strong>通信轮数</strong>。</li>
<li>为了得到平滑的学习曲线，他们对每个参数组合都进行了学习率 <code>η</code> 的优化，并将学习曲线处理为单调递增（即只记录历史最佳准确率）。</li>
<li>他们通过线性插值，计算出学习曲线<strong>首次达到目标准确率</strong>时所对应的通信轮数。</li>
<li>参考<strong>Figure 2</strong>（图2）来直观理解，图中的灰色水平线代表了Table 1中的目标准确率。</li>
</ul>
</li>
<li><strong>发现</strong>：<ul>
<li><strong>当 <code>B=∞</code>（大批次）时，增加 <code>C</code> 效果甚微</strong>：无论是IID还是非IID数据，将 <code>C</code> 从0.1增加到1.0，通信轮数几乎没有变化，甚至有时会增加。这说明单纯地让更多客户端参与，但每个客户端只做一次梯度计算（<code>B=∞</code>），并不能有效加速。</li>
<li><strong>当 <code>B</code> 较小（小批次）时，增加 <code>C</code> 能带来显著加速</strong>：特别是对于非IID数据，当 <code>C</code> 从0.1增加到1.0时，通信轮数大幅下降（如MNIST CNN在非IID数据上，从206轮降到97轮）。这是因为<u>小批次SGD引入了随机性，多个客户端的随机梯度平均后能更好地逼近真实梯度。</u></li>
<li><strong>存在“边际效应递减”</strong>：从 <code>C=0.1</code> 增加到 <code>C=0.2</code> 或 <code>C=0.5</code> 时，加速效果明显；但从 <code>C=0.5</code> 增加到 <code>C=1.0</code> 时，收益变小。这说明并行度并非越高越好，增加到一定程度后，收益会递减。</li>
</ul>
</li>
<li><strong>结论</strong>：<ul>
<li><strong>确定 <code>C</code> 的默认值</strong>：基于以上结果，作者决定在后续实验中将 <code>C</code> 固定为 <code>0.1</code>。这是一个<strong>权衡</strong>的结果：<code>C=0.1</code> 既能获得不错的并行加速，又不会因为协调过多客户端而带来过高的系统开销。</li>
<li><strong>引出下一节</strong>：作者指出，表1中观察到的巨大加速效果（尤其是在 <code>B=10</code> 时），其主要驱动力并非来自 <code>C</code>，“<strong>单纯增加并行度</strong>（<code>C</code>）”在联邦学习中效果有限。真正的效率提升来自<strong>增加每个客户端的计算量</strong>（即减小 <code>B</code>）。</li>
</ul>
</li>
</ol>
<h2 id="增加每个客户端的计算量（并未看懂）"><a href="#增加每个客户端的计算量（并未看懂）" class="headerlink" title="增加每个客户端的计算量（并未看懂）"></a>增加每个客户端的计算量（并未看懂）</h2><ol>
<li><strong>方法</strong>：<ul>
<li>固定上一节确定的并行度 <code>C=0.1</code>。<strong>增加每个客户端的计算量</strong>：<strong>减小本地小批量大小 <code>B</code></strong> 和&#x2F;或<strong>增加本地训练轮数 <code>E</code></strong>。</li>
<li>引入了一个关键指标 <code>u = nE/(KB)</code>来量化计算量：它代表了每轮通信中，每个客户端预期执行的SGD更新次数。<code>u</code> 越大，意味着本地计算量越大。</li>
</ul>
</li>
<li><strong>发现一：通信效率大幅提升</strong><ul>
<li><strong>主要结论</strong>：通过增加 <code>u</code>（即增加本地计算量），可以<strong>极大地减少</strong>达到目标准确率所需的通信轮次。</li>
<li>数据支持：表2（Table 2）<ul>
<li>在<strong>IID数据</strong>上，加速效果极为显著。例如，在MNIST CNN上，FedAvg (<code>E=20, B=10</code>) 比FedSGD快了<strong>34.8倍</strong>。</li>
<li>在<strong>非IID数据</strong>上，加速效果虽然较小（如MNIST CNN上为2.8倍），但依然显著，证明了算法的<strong>鲁棒性</strong>。</li>
<li>在<strong>真实世界模拟</strong>（莎士比亚LSTM）上，加速效果最为惊人，达到了<strong>95.3倍</strong>。这是因为数据不平衡，拥有大量数据的“大客户”通过多轮本地训练获得了巨大收益。</li>
</ul>
</li>
</ul>
</li>
<li><strong>发现二：FedAvg能训练出性能更高的模型</strong><ul>
<li><strong>结论</strong>：FedAvg不仅更快，而且最终训练出的模型<strong>测试精度更高</strong>。</li>
<li><strong>举例</strong>：在MNIST CNN上，FedSGD最终精度为99.22%，而FedAvg在更短时间内达到了99.44%。</li>
<li><strong>解释</strong>：作者推测，周期性的模型平均过程产生了一种<strong>正则化效应</strong>，类似于“Dropout”技术。这有助于防止模型过拟合到某个客户端的局部数据上，从而提升了模型的泛化能力。</li>
</ul>
</li>
</ol>
<h2 id="我们可以在客户端数据集上过度优化吗？"><a href="#我们可以在客户端数据集上过度优化吗？" class="headerlink" title="我们可以在客户端数据集上过度优化吗？"></a>我们可以在客户端数据集上过度优化吗？</h2><p><strong>本地训练不能无限进行</strong></p>
<ul>
<li>如果让一个客户端在本地训练过多轮（<code>E</code> 过大），全局模型的性能会<strong>停滞甚至发散</strong>。</li>
<li><strong>解释</strong>：如果一个客户端在本地训练太久，其模型会严重<strong>过拟合</strong>于自己那部分有限的本地数据。当这个“过拟合”的模型被聚合时，会损害全局模型的质量。</li>
<li><strong>结论</strong>：<code>E</code> 是一个需要调整的超参数。在训练后期，可能需要像<strong>衰减学习率</strong>一样，<strong>减小 <code>E</code> 或增大 <code>B</code></strong>，以避免过拟合。</li>
</ul>
<p><strong>结论</strong>：在实践中，应优先调整 <code>B</code>（小批量大小）。因为只要 <code>B</code> 足够大以利用硬件并行性，减小 <code>B</code> 带来的计算时间成本几乎为零，但能显著增加 <code>u</code>，从而<strong>降低通信轮次</strong>。</p>
<h2 id="CIFAR实验"><a href="#CIFAR实验" class="headerlink" title="CIFAR实验"></a><strong>CIFAR</strong>实验</h2><p>旨在通过一个更复杂、更标准的基准数据集（CIFAR-10）来<strong>进一步验证FedAvg算法的有效性</strong>，并将其与更广泛的基线进行比较。</p>
<ol>
<li><strong>实验设置</strong>：<ul>
<li><strong>数据集</strong>：CIFAR-10，包含10类32x32的彩色图片。</li>
<li><strong>数据划分</strong>：将数据平均分配给100个客户端，形成一个<strong>平衡且IID</strong>的数据集（因为没有自然的用户划分）。</li>
<li><strong>模型</strong>：采用一个标准的CNN模型（两个卷积层+两个全连接层），约100万个参数。</li>
<li><strong>基线</strong>：引入了一个强大的新基线——<strong>标准SGD</strong>：在不划分客户端的情况下，将所有数据集中起来进行训练。这是<strong>理论上性能最好</strong>的情况。</li>
</ul>
</li>
<li><strong>发现一：FedAvg在通信效率上优势巨大</strong><ul>
<li><strong>与联邦基线（FedSGD）对比</strong>：表3（Table 3）显示，要达到80%的准确率，FedSGD需要3750轮，而FedAvg仅需<strong>280轮</strong>，通信轮次减少了<strong>64倍</strong>。</li>
<li><strong>与集中式SGD对比</strong>：标准SGD需要进行197,500次小批量更新才能达到86%的准确率。而在联邦学习中，每次小批量更新都对应一次通信。FedAvg仅用<strong>2,000次通信</strong>就达到了85%的准确率。这证明了FedAvg在<strong>通信效率</strong>上的巨大成功。</li>
</ul>
</li>
<li><strong>发现二：FedAvg在计算效率上也表现良好</strong><ul>
<li>作者将比较的维度从“通信轮次”扩展到“<strong>小批量梯度计算次数</strong>”（这是衡量计算成本的指标）。</li>
<li>通过设置 <code>B=50</code>，他们发现，在相同的小批量计算次数下，FedAvg的性能<strong>与标准SGD非常接近</strong>（见附录图9）。这表明，FedAvg不仅通信效率高，而且在利用计算资源方面也非常高效。</li>
</ul>
</li>
<li><strong>发现三：模型平均能平滑训练过程</strong><ul>
<li>实验观察到，无论是标准SGD还是只使用一个客户端的FedAvg，其训练过程中的准确率都会出现<strong>显著的波动</strong>（oscillations）。</li>
<li>而使用FedAvg（<code>C=0.1</code>）时，由于聚合了多个客户端的更新，训练曲线变得更加<strong>平滑</strong>。这说明模型平均过程具有稳定训练的作用。</li>
</ul>
</li>
</ol>
<h2 id="大规模的LSTM实验"><a href="#大规模的LSTM实验" class="headerlink" title="大规模的LSTM实验"></a>大规模的LSTM实验</h2><p>在一个拥有超过50万用户的社交网络上，利用用户的公开发帖数据来训练一个“智能输入法”模型，用于预测下一个词，旨在将FedAvg算法应用于一个规模巨大、数据分布高度非IID的真实世界模拟场景，以证明其在<strong>实际应用</strong>中的强大有效性。</p>
<ol>
<li><strong>实验设置</strong>：<ul>
<li><strong>数据集</strong>：使用了一个包含1000万条公开帖子的大型社交网络数据集。</li>
<li><strong>客户端划分</strong>：<strong>按作者分组</strong>，每个作者被视为一个独立的客户端。这完美地模拟了联邦学习的“自然划分”。</li>
<li><strong>数据特性</strong>：数据集具有<strong>极度的非IID和不平衡</strong>特性。每个用户的语言风格、用词习惯、发帖数量都千差万别。</li>
<li><strong>模型</strong>：一个规模较大的词级LSTM模型（约500万参数），用于下一个词预测。</li>
<li><strong>实验规模</strong>：每轮从超过50万客户端中选择200个参与，这是一个非常典型的联邦学习场景。</li>
</ul>
</li>
<li><strong>发现</strong>：<ul>
<li><strong>通信效率显著</strong>：FedAvg (<code>E=1, B=8</code>) 仅用<strong>35轮</strong>通信就达到了10.5%的准确率，而FedSGD需要<strong>820轮</strong>。这相当于<strong>23倍的通信轮次减少</strong>。这再次证明了FedAvg的巨大优势。</li>
<li><strong>训练过程更稳定</strong>：FedAvg的测试准确率<strong>方差更低</strong>，学习曲线更平滑。这表明模型平均过程有助于稳定训练，减少因客户端数据差异带来的波动。</li>
<li><strong><code>E</code> 值并非越大越好</strong>：与在莎士比亚数据集上的发现一致，实验发现 <code>E=1</code> 的性能<strong>略优于</strong> <code>E=5</code>。这再次印证了“过度本地训练可能导致过拟合”的结论。在这个大规模、高度异质的场景下，进行多轮本地训练反而不如只进行一轮本地更新有效。</li>
</ul>
</li>
</ol>
<h1 id="结论和未来工作"><a href="#结论和未来工作" class="headerlink" title="结论和未来工作"></a>结论和未来工作</h1><h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h3><p>各种模型架构（多层感知器，两种不同的卷积神经网络， 两层字符LSTM和大规模词级LSTM）的实验结果表明：当FedAvg使用<strong>相对较少的交流轮次来训练高质量的模型时，联邦学习是实际可行的</strong>。</p>
<h3 id="未来工作"><a href="#未来工作" class="headerlink" title="未来工作"></a>未来工作</h3><ul>
<li><strong>差分隐私 (Differential Privacy)</strong>：通过在模型更新中添加噪声，来保证任何单个用户的数据都无法被准确推断。</li>
<li><strong>安全多方计算 (Secure Multi-Party Computation)</strong>：允许多个参与方在不泄露各自私有输入的情况下，共同计算一个函数（如模型平均），使得服务器也无法看到单个客户端的更新。</li>
</ul>
<p>行文思路是梳理清楚了（借助了网上其他佬的学习笔记和Q老师的辅助），但是很多实验细节我都看不懂😭，去补点ML基础吧。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>Author: </span><span class="post-copyright-info"><a href="https://rebekahlan.github.io">RebekahLAN</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>Link: </span><span class="post-copyright-info"><a href="https://rebekahlan.github.io/2025/08/20/FL%E5%BC%80%E5%B1%B1%E4%B9%8B%E4%BD%9C%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/">https://rebekahlan.github.io/2025/08/20/FL%E5%BC%80%E5%B1%B1%E4%B9%8B%E4%BD%9C%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>Copyright Notice: </span><span class="post-copyright-info">All articles on this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless otherwise stated.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Federated-Learning/">Federated Learning</a></div><div class="post-share"><div class="social-share" data-image="/images/background.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related full-width" href="/2025/08/10/Hexo-Github-pages%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2-%E8%AE%B0%E5%BD%95/" title="Hexo+Github pages搭建个人博客 记录:)"><img class="cover" src="/images/background.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="info text-right"><div class="info-1"><div class="info-item-1">Next</div><div class="info-item-2">Hexo+Github pages搭建个人博客 记录:)</div></div><div class="info-2"><div class="info-item-1">一、准备工作1.注册Github账号之前已完成。 大一搞的。记得当时在b站上找了一个手写学生证明的教程，非常有用xd. 2.安装Node.js(带npm)之前已完成。 竟然是前几天作品赛队友发给我的，刚好用上了😋。 安装后验证： 12node -vnpm -v  3.安装Git之前已完成。 安装后验证： 1git --version  4.配置Git用户信息之前已完成。 12git config --global user.name &quot;你的GitHub用户名&quot;git config --global user.email &quot;你的邮箱&quot;  验证： 12git config --global user.namegit config --global user.email  5. SSH 密钥配置生成SSH密钥 打开Git Bash： 1ssh-keygen -t ed25519 -C &quot;你的邮箱地址&quot;  连续回车三次即可。   Tip:  如果你和博主一样想把密钥存在自己记得的位置， 在提示 Enter file in wh...</div></div></div></a></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/images/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">RebekahLAN</div><div class="author-info-description">Blogs of RebekahLAN, an artist of science, nature, life and love.</div><div class="site-data"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">3</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">2</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">2</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Contents</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%A0%87%E9%A2%98"><span class="toc-number">1.</span> <span class="toc-text">标题</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%91%98%E8%A6%81"><span class="toc-number">2.</span> <span class="toc-text">摘要</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%BC%95%E8%A8%80"><span class="toc-number">3.</span> <span class="toc-text">引言</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%83%8C%E6%99%AF"><span class="toc-number">3.1.</span> <span class="toc-text">背景</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%81%94%E9%82%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%AE%9A%E4%B9%89"><span class="toc-number">3.2.</span> <span class="toc-text">联邦学习的定义</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%BB%E8%A6%81%E8%B4%A1%E7%8C%AE"><span class="toc-number">3.3.</span> <span class="toc-text">主要贡献</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%81%94%E9%82%A6%E5%AD%A6%E4%B9%A0%E9%80%82%E7%94%A8%E5%B1%9E%E6%80%A7"><span class="toc-number">3.4.</span> <span class="toc-text">联邦学习适用属性</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%9A%90%E7%A7%81"><span class="toc-number">3.5.</span> <span class="toc-text">隐私</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%81%94%E9%82%A6%E4%BC%98%E5%8C%96"><span class="toc-number">3.6.</span> <span class="toc-text">联邦优化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B9%8B%E5%89%8D%E7%9A%84%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C"><span class="toc-number">3.7.</span> <span class="toc-text">之前的相关工作</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%81%94%E9%82%A6%E5%AD%A6%E4%B9%A0%E5%B9%B3%E5%9D%87%E7%AE%97%E6%B3%95"><span class="toc-number">4.</span> <span class="toc-text">联邦学习平均算法</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#FedSGD"><span class="toc-number">4.1.</span> <span class="toc-text">FedSGD</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#FedAvg"><span class="toc-number">4.2.</span> <span class="toc-text">FedAvg</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#FedAvg%E4%B8%BA%E4%BB%80%E4%B9%88%E6%9C%89%E6%95%88%EF%BC%9F"><span class="toc-number">4.3.</span> <span class="toc-text">FedAvg为什么有效？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#FedAvg-%E4%B8%8E-FedSGD-%E7%9A%84%E5%85%B3%E7%B3%BB"><span class="toc-number">4.4.</span> <span class="toc-text">FedAvg 与 FedSGD 的关系</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C"><span class="toc-number">5.</span> <span class="toc-text">实验结果</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">5.1.</span> <span class="toc-text">数据集</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#MNIST%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E8%AF%86%E5%88%AB"><span class="toc-number">5.1.1.</span> <span class="toc-text">MNIST手写数字识别</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%8E%8E%E5%A3%AB%E6%AF%94%E4%BA%9A%E6%96%87%E9%9B%86"><span class="toc-number">5.1.2.</span> <span class="toc-text">莎士比亚文集</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A2%9E%E5%8A%A0%E5%B9%B6%E8%A1%8C%E6%80%A7"><span class="toc-number">5.2.</span> <span class="toc-text">增加并行性</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A2%9E%E5%8A%A0%E6%AF%8F%E4%B8%AA%E5%AE%A2%E6%88%B7%E7%AB%AF%E7%9A%84%E8%AE%A1%E7%AE%97%E9%87%8F%EF%BC%88%E5%B9%B6%E6%9C%AA%E7%9C%8B%E6%87%82%EF%BC%89"><span class="toc-number">5.3.</span> <span class="toc-text">增加每个客户端的计算量（并未看懂）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%88%91%E4%BB%AC%E5%8F%AF%E4%BB%A5%E5%9C%A8%E5%AE%A2%E6%88%B7%E7%AB%AF%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%8A%E8%BF%87%E5%BA%A6%E4%BC%98%E5%8C%96%E5%90%97%EF%BC%9F"><span class="toc-number">5.4.</span> <span class="toc-text">我们可以在客户端数据集上过度优化吗？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#CIFAR%E5%AE%9E%E9%AA%8C"><span class="toc-number">5.5.</span> <span class="toc-text">CIFAR实验</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%A7%E8%A7%84%E6%A8%A1%E7%9A%84LSTM%E5%AE%9E%E9%AA%8C"><span class="toc-number">5.6.</span> <span class="toc-text">大规模的LSTM实验</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%BB%93%E8%AE%BA%E5%92%8C%E6%9C%AA%E6%9D%A5%E5%B7%A5%E4%BD%9C"><span class="toc-number">6.</span> <span class="toc-text">结论和未来工作</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%93%E8%AE%BA"><span class="toc-number">6.0.1.</span> <span class="toc-text">结论</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9C%AA%E6%9D%A5%E5%B7%A5%E4%BD%9C"><span class="toc-number">6.0.2.</span> <span class="toc-text">未来工作</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Posts</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/08/20/FL%E5%BC%80%E5%B1%B1%E4%B9%8B%E4%BD%9C%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/" title="FL开山之作阅读笔记">FL开山之作阅读笔记</a><time datetime="2025-08-20T06:33:55.000Z" title="Created 2025-08-20 14:33:55">2025-08-20</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/08/10/Hexo-Github-pages%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2-%E8%AE%B0%E5%BD%95/" title="Hexo+Github pages搭建个人博客 记录:)">Hexo+Github pages搭建个人博客 记录:)</a><time datetime="2025-08-09T16:30:11.000Z" title="Created 2025-08-10 00:30:11">2025-08-10</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/08/07/My-first-blog/" title="My first blog">My first blog</a><time datetime="2025-08-07T08:51:43.000Z" title="Created 2025-08-07 16:51:43">2025-08-07</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url(/images/background.jpg);"><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;&nbsp;2025 By RebekahLAN</span><span class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 7.3.0</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.4.3</a></span></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Reading Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Toggle Between Light and Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle Between Single-column and Double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="Settings"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back to Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>